## L1与L2正则化深度对比

L1和L2是两种最常见的正则化技术，它们通过在损失函数中添加不同的惩罚项来防止过拟合。它们的核心区别在于**惩罚方式的不同**，这导致了它们在模型训练和结果上截然不同的特性。

### 1. 数学公式的差异

-   **L1 正则化 (Lasso Regression)**: 惩罚项是所有模型参数 $w_j$ **绝对值之和**。
    $L_{total} = L_{original} + \lambda \sum_{j=1}^{m} |w_j|$

-   **L2 正则化 (Ridge Regression)**: 惩罚项是所有模型参数 $w_j$ **平方和**。
    $L_{total} = L_{original} + \lambda \sum_{j=1}^{m} w_j^2$

这个看似微小的差异（绝对值 vs 平方）导致了后续所有特性的不同。

### 2. 几何直觉：为什么 L1 能产生稀疏解？

这是两者最核心、最直观的区别。我们可以通过一个二维的例子来理解（假设模型只有两个参数 $w_1$ 和 $w_2$）。模型优化的目标可以看作是在一个约束条件下，找到使原始损失 $L_{original}$ 最小的点。

-   **约束条件**:
    -   对于 L1，正则化项 $|w_1| + |w_2| \le C$ 在坐标系中构成一个**菱形（Diamond）**。
    -   对于 L2，正则化项 $w_1^2 + w_2^2 \le C$ 在坐标系中构成一个**圆形（Circle）**。

-   **优化过程**:
    -   原始损失函数 $L_{original}$ 的等高线（contour lines）通常是一系列的**椭圆**，椭圆的中心是未经正则化时的最优解。
    -   优化的过程，就是寻找这个椭圆与正则化项构成的“约束区域”（菱形或圆形）**首次相交**的那个点。

**关键区别**：

-   **L1 (菱形)**: 菱形有尖锐的**角点**，这些角点正好位于坐标轴上。当损失函数的椭圆等高线扩大并接触菱形时，有**极大的概率**会首先接触到其中一个角点。而只要接触点在角点上，就意味着其中一个参数（$w_1$ 或 $w_2$）为 **0**。这就是 L1 正则化能够产生**稀疏解（Sparse Solution）**的几何原因。

-   **L2 (圆形)**: 圆形是完全平滑的，没有角点。当椭圆与圆形相交时，接触点可以是圆周上的**任何一点**，这个点恰好在坐标轴上的概率极低。因此，L2 正则化只会让参数**趋近于 0**，但很难使其**精确地等于 0**。

**一句话总结几何直觉：L1 的菱形约束有“角”，更容易与损失函数的等高线在坐标轴上相交，从而产生稀疏性；L2 的圆形约束是平滑的，交点几乎不可能在坐标轴上。**

### 3. 优化过程的差异：梯度视角

从参数更新的梯度下降角度看，更能理解其内在机制。

$\frac{\partial L_{total}}{\partial w_j} = \frac{\partial L_{original}}{\partial w_j} + \lambda \frac{\partial R(w_j)}{\partial w_j}$

-   **L2 的梯度**: $\frac{\partial R_{L2}}{\partial w_j} = 2w_j$。惩罚项的梯度与参数 $w_j$ 大小成正比。$w_j$ 越大，惩罚越强；$w_j$ 越小，惩罚越弱。这使得 $w_j$ 很难被精确地“刹住”在 0 点。

-   **L1 的梯度**: $\frac{\partial R_{L1}}{\partial w_j} = \text{sgn}(w_j)$。惩罚项的梯度是一个**常数**（$\lambda$ 或 $-\lambda$），与参数 $w_j$ 本身的大小无关。无论参数值有多小，它都会被一个**恒定的力量**推向 0，因此更容易将参数固定在 0。

### 4. 实际应用与总结

| 特性 | L1 正则化 (Lasso) | L2 正则化 (Ridge) |
| :--- | :--- | :--- |
| **核心效果** | **稀疏性 (Sparsity)** | **权重衰减 (Weight Decay)** |
| **参数结果** | 产生稀疏解，很多参数为 0 | 参数值变得很小，但几乎不为 0 |
| **主要用途** | **特征选择 (Feature Selection)** | **防止过拟合**，处理多重共线性 |
| **计算复杂度** | 较高（在0点不可导） | 较低（平滑可导） |
| **解的稳定性** | 较低 | 较高 |

**何时使用？**

-   **使用 L1 (Lasso)**: 当你认为输入特征中只有一小部分是真正重要的，希望自动完成特征筛选时。
-   **使用 L2 (Ridge)**: 当你认为所有特征都有用，希望提高模型通用性和稳健性时。这是更常用的默认选项。

### 5. 弹性网络 (Elastic Net)

弹性网络结合了 L1 和 L2 的优点，对于处理特征高度相关且需要特征选择的场景非常有效。

$L_{total} = L_{original} + \lambda_1 \sum_{j=1}^{m} |w_j| + \lambda_2 \sum_{j=1}^{m} w_j^2$
